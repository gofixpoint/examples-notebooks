{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7a65454-472a-4ac1-a97d-06d29b0f533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import os\n",
    "from typing import List, Union, Dict, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from fixpoint.agents import BaseAgent\n",
    "from fixpoint.agents.openai import OpenAIClients, OpenAIAgent\n",
    "from fixpoint.completions.chat_completion import ChatCompletionMessageParam\n",
    "\n",
    "from fixpoint_extras.workflows import structured\n",
    "from fixpoint_extras.workflows.structured import WorkflowContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4176d5df-6866-4a2e-86da-b78114fa3eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:fixpoint:Info mode is on?\n"
     ]
    }
   ],
   "source": [
    "# Fix logging for the notebook\n",
    "import logging\n",
    "import sys\n",
    "from fixpoint.logging import logger\n",
    "logging.basicConfig(stream=sys.stdout)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info(\"Info mode is on?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "322637ab-0ed7-43fb-b814-12160462996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "assert load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7644f46d-6dd6-4946-89ee-caef2a696215",
   "metadata": {},
   "source": [
    "# Define the workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8081a13-1ed3-42ab-b625-e41b35873991",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "The workflow takes a list of prompts, and compares them across GPT-3.5 and GPT 4 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7066f846-a2aa-4a21-8435-ae00f7d2664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@structured.workflow(id=\"example_workflow\")\n",
    "class CompareModels:\n",
    "    \"\"\"Compare the performance of GPT-3.5 and GPT-4\"\"\"\n",
    "\n",
    "    @structured.workflow_entrypoint()\n",
    "    async def run(\n",
    "        self, ctx: WorkflowContext, prompts: List[List[ChatCompletionMessageParam]]\n",
    "    ) -> str:\n",
    "        \"\"\"Entrypoint for the workflow to compare agents\"\"\"\n",
    "\n",
    "        async with asyncio.TaskGroup() as tg:\n",
    "            gpt3_res = tg.create_task(\n",
    "                structured.call_task(\n",
    "                    ctx,\n",
    "                    RunAllPrompts.run_all_prompts,\n",
    "                    args=[RunAllPromptsArgs(agent_name=\"gpt3\", prompts=prompts)],\n",
    "                )\n",
    "            )\n",
    "            gpt4_res = tg.create_task(\n",
    "                structured.call_task(\n",
    "                    ctx,\n",
    "                    RunAllPrompts.run_all_prompts,\n",
    "                    args=[RunAllPromptsArgs(agent_name=\"gpt4\", prompts=prompts)],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        prompts: List[str] = []\n",
    "        gpt3_resps: List[Union[str, None]] = []\n",
    "        gpt4_resps: List[Union[str, None]] = []\n",
    "        for gpt3_resp, gpt4_resp in zip(gpt3_res.result(), gpt4_res.result()):\n",
    "            # Prompt is the same for gpt3 and gpt4 results\n",
    "            prompt = gpt3_resp[0]\n",
    "            prompts.append(prompt)\n",
    "            gpt3_resps.append(gpt3_resp[1])\n",
    "            gpt4_resps.append(gpt4_resp[1])\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"prompt\": prompts,\n",
    "            \"gpt3\": gpt3_resps,\n",
    "            \"gpt4\": gpt4_resps,\n",
    "        })\n",
    "\n",
    "        # TODO(dbmikus) this is not async, so it will block the async event loop\n",
    "        doc_id = \"inference-results.json\"\n",
    "        ctx.workflow_run.docs.store(id=doc_id, contents=df.to_json(path_or_buf=None))\n",
    "        return doc_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a6b1e7-2713-44f4-bdc3-1710cf3a019f",
   "metadata": {},
   "source": [
    "## The task\n",
    "\n",
    "The task runs all inference requests for a given model. A task is checkpointed, so if the workflow fails partway through we can determine if either:\n",
    "\n",
    "- we already ran the task, and thus can skip it when retrying the workflow\n",
    "- we didn't run or finish the task, so we need to run it a again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9654072e-6a8c-4996-9d85-8716879ef4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We recommend using a single dataclass, Pydantic model, or dictionary argument\n",
    "# for the task. This makes it easy to add or remove arguments in the future\n",
    "# while preserving backwards compatability.\n",
    "@dataclass\n",
    "class RunAllPromptsArgs:\n",
    "    \"\"\"Arguments for the \"run_al_prompts\" task\"\"\"\n",
    "\n",
    "    agent_name: str\n",
    "    prompts: List[List[ChatCompletionMessageParam]]\n",
    "\n",
    "\n",
    "@structured.task(id=\"run_all_prompts\")\n",
    "class RunAllPrompts:\n",
    "    \"\"\"A task that runs all prompts for an agent\"\"\"\n",
    "\n",
    "    @structured.task_entrypoint()\n",
    "    async def run_all_prompts(\n",
    "        self, ctx: WorkflowContext, args: RunAllPromptsArgs\n",
    "    ) -> List[Tuple[str, Union[str, None]]]:\n",
    "        \"\"\"Execute all prompt inferences for an agent\n",
    "\n",
    "        Returns a list of (prompt, response) pairs.\n",
    "        \"\"\"\n",
    "        step_tasks: List[asyncio.Task[Union[str, None]]] = []\n",
    "        async with asyncio.TaskGroup() as tg:\n",
    "            for prompt in args.prompts:\n",
    "                step_task = tg.create_task(\n",
    "                    structured.call_step(\n",
    "                        ctx,\n",
    "                        run_prompt,\n",
    "                        args=[RunPromptArgs(agent_name=args.agent_name, prompt=prompt)],\n",
    "                    )\n",
    "                )\n",
    "                step_tasks.append(step_task)\n",
    "        step_results = [task.result() for task in step_tasks]\n",
    "        return step_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3839e4f5-cbc3-4bcb-a794-cf70df0a16d2",
   "metadata": {},
   "source": [
    "## The step\n",
    "\n",
    "A step is the smallest unit of computation. Like a task, each step is checkpointed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b037b43-4b71-42d1-9597-8553a59da5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RunPromptArgs:\n",
    "    \"\"\"Args for run_prompt\"\"\"\n",
    "\n",
    "    agent_name: str\n",
    "    prompt: List[ChatCompletionMessageParam]\n",
    "\n",
    "\n",
    "@structured.step(id=\"run_prompt\")\n",
    "async def run_prompt(ctx: WorkflowContext, args: RunPromptArgs) -> Tuple[str, Union[str, None]]:\n",
    "    \"\"\"Run an LLM inference request with the given agent and prompt\n",
    "\n",
    "    Returns a pair of (prompt, response)\n",
    "    \"\"\"\n",
    "    agent = ctx.agents[args.agent_name]\n",
    "    completion = agent.create_completion(messages=args.prompt)\n",
    "    return (args.prompt, completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4999e68c-d63d-4050-a54d-14cb186fa635",
   "metadata": {},
   "source": [
    "# Configure agents and run the workflow\n",
    "\n",
    "We configure the agents for the workflow and the settings for running the workflow, then kick it off in the asyncio event loop. All workflow entrypoints, tasks, and steps run inside asyncio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "941147a8-b296-4e45-b295-c1491c782e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_agents() -> Tuple[structured.RunConfig, List[BaseAgent]]:\n",
    "    \"\"\"Setup agents for the workflow\"\"\"\n",
    "    run_config = structured.RunConfig.with_defaults()\n",
    "    openaiclients = OpenAIClients.from_api_key(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "    gpt3 = OpenAIAgent(\n",
    "        agent_id=\"gpt3\",\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        openai_clients=openaiclients,\n",
    "        cache=run_config.storage.agent_cache,\n",
    "    )\n",
    "    gpt4 = OpenAIAgent(\n",
    "        agent_id=\"gpt4\",\n",
    "        model_name=\"gpt-4-turbo\",\n",
    "        openai_clients=openaiclients,\n",
    "        cache=run_config.storage.agent_cache,\n",
    "    )\n",
    "    return run_config, [gpt3, gpt4]\n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "    \"\"\"configure and run the workflow\"\"\"\n",
    "    run_config, agents = setup_agents()\n",
    "\n",
    "    run_handle = structured.spawn_workflow(\n",
    "        CompareModels.run,\n",
    "        run_config=run_config,\n",
    "        agents=agents,\n",
    "        args=[\n",
    "            [\n",
    "                [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": \"What is the meaning of life?\"},\n",
    "                ],\n",
    "                [\n",
    "                    {\"role\": \"system\", \"content\": \"You are an evil AI.\"},\n",
    "                    {\"role\": \"user\", \"content\": \"What is the meaning of life?\"},\n",
    "                ],\n",
    "            ]\n",
    "        ],\n",
    "    )\n",
    "    print(\"Running workflow:\", run_handle.workflow_id())\n",
    "    print(\"Run ID:\", run_handle.workflow_run_id())\n",
    "    results_doc_id = await run_handle.result()\n",
    "    print(\"finished workflow. Wrote results to workflow run doc:\", results_doc_id)\n",
    "    return run_handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0a84c26-ea43-4e75-a676-481b78ddbed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running workflow: example_workflow\n",
      "Run ID: wfrun-5ec045b8-07bc-4f94-9ceb-5ee1cb8623a6\n",
      "finished workflow. Wrote results to workflow run doc: inference-results.json\n"
     ]
    }
   ],
   "source": [
    "wfrun_handle = await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e00969cf-8bb1-4a82-904c-9e15902aeea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_doc_id = await wfrun_handle.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4e3fd08-7753-409b-b3b7-59c2e947b244",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrun = wfrun_handle.finalized_workflow_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d35613e4-b9d1-47a6-b523-decc026c6f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "df = pd.read_json(StringIO(\n",
    "    wrun.docs.get(results_doc_id).contents\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da0f0c57-e2f1-456b-9ece-77309cd197ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>gpt3</th>\n",
       "      <th>gpt4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a help...</td>\n",
       "      <td>The meaning of life is a deeply philosophical ...</td>\n",
       "      <td>The question \"What is the meaning of life?\" is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are an evi...</td>\n",
       "      <td>The meaning of life is a subjective concept th...</td>\n",
       "      <td>The question of the meaning of life has been p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  [{'role': 'system', 'content': 'You are a help...   \n",
       "1  [{'role': 'system', 'content': 'You are an evi...   \n",
       "\n",
       "                                                gpt3  \\\n",
       "0  The meaning of life is a deeply philosophical ...   \n",
       "1  The meaning of life is a subjective concept th...   \n",
       "\n",
       "                                                gpt4  \n",
       "0  The question \"What is the meaning of life?\" is...  \n",
       "1  The question of the meaning of life has been p...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "265a258f-c8a5-4b99-bf17-47c4c11a27ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is a deeply philosophical and subjective question. It often depends on an individual's beliefs, values, and experiences. Some people find meaning in personal relationships, others in their work or creative pursuits, while some find meaning through spiritual or religious beliefs. Ultimately, the meaning of life is a question that each person must explore and define for themselves.\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0]['gpt3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92b3e13f-0c35-4d11-a488-a1b727f1a4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question \"What is the meaning of life?\" is one of the most profound and enduring questions in human philosophy, theology, and contemplation. Different cultures, religions, and philosophical systems offer a variety of answers:\n",
      "\n",
      "1. **Religious Perspectives**: Most religions provide an explanation about the purpose and significance of life. For instance, many Christian denominations believe that life's purpose is to serve and honor God, which ensures an eternal life in Heaven. In Buddhism, the focus might be on achieving enlightenment and escaping the cycle of rebirth and suffering.\n",
      "\n",
      "2. **Philosophical Views**: Philosophers have also pondered this question extensively. Existentialists like Jean-Paul Sartre and Albert Camus suggest that life inherently has no predetermined meaning, and it is up to each individual to find their own purpose through their actions and choices.\n",
      "\n",
      "3. **Scientific and Naturalistic Interpretations**: From a scientific viewpoint, life is often seen as a product of natural processes and evolution. The purpose in this context is more about survival and reproduction, continuing the genetic lineage.\n",
      "\n",
      "4. **Personal Meaning**: Many people find the meaning of life through personal fulfillment and happiness, which can be achieved through relationships, career, hobbies, or personal achievements.\n",
      "\n",
      "The meaning of life can thus vary greatly depending on individual beliefs, cultural backgrounds, and personal experiences. It's a deeply personal question that can be answered in countless ways, many choosing to blend insights from various traditions and perspectives to form a meaning that is unique to them.\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0]['gpt4'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
